---
title: "Lab02_communties"
author: "Julia Parish"
date: "1/31/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Load and plot the `penguins` dataset

The `penguins` dataset comes from Allison Horst's [palmerpenguins](https://allisonhorst.github.io/palmerpenguins) R package and records biometric measurements of different penguin species found at Palmer Station, Antarctica [@gormanEcologicalSexualDimorphism2014]. It is an alternative to the `iris` dataset example for exploratory data analysis (to avoid association of this 1935 dataset's collector [Ronald Fisher](https://en.wikipedia.org/wiki/Ronald_Fisher) who "held strong views on race and eugenics"). Use of either dataset will be acceptable for submission of this lab (and mention of `iris` or Fisher will be dropped for next year).

```{=html}
<style>
.row {
  display: flex;
}
.column {
  flex: 50%;
  padding: 5px;
}
</style>
<div class="row">
  <div class="column">
    <img src="https://allisonhorst.github.io/palmerpenguins/reference/figures/lter_penguins.png" style="width:100%"/>
  </div>
  <div class="column">
    <img src="https://allisonhorst.github.io/palmerpenguins/reference/figures/culmen_depth.png" style="width:100%"/>
  </div>
</div>
```

```{r}
# load R packages
librarian::shelf(cluster, dplyr, DT, factoextra, ggplot2, ggvoronoi, scales, h2o, palmerpenguins, tibble, vegan, vegan3d)

# set seed for reproducible results
set.seed(42)

```

```{r}
# load the dataset
data("penguins")

# look at documentation in RStudio
if (interactive())
  help(penguins)

# show data table
datatable(penguins)

# skim the table for a summary
skim(penguins)

# remove the rows with NAs
penguins <- na.omit(penguins)

```

```{r}
# plot length vs depth, species naive
ggplot(penguins, aes(bill_length_mm, bill_depth_mm)) +
  geom_point()

# plot length vs depth, color by species
legend_pos <- theme(
    legend.position = c(0.95, 0.05),
    legend.justification = c("right", "bottom"),
    legend.box.just = "right")

ggplot(penguins, aes(bill_length_mm, bill_depth_mm, color = species)) +
  geom_point() +
  legend_pos
```

### Cluster `penguins` using `kmeans()`

```{r}
# cluster using kmeans
k <- 3  # number of clusters

penguins_k <- kmeans(penguins %>% 
    select(bill_length_mm, bill_depth_mm), 
  centers = k)

# show cluster result
penguins_k

```

```{r}
# compare clusters with species (which were not used to cluster)
table(penguins_k$cluster, penguins$species)
```

**Bonus Question**: How many observations could be considered "misclassified" if expecting bill length and bill depth to differentiate between species?


```{r}
# extract cluster assignment per observation
Cluster = factor(penguins_k$cluster)
ggplot(penguins, aes(bill_length_mm, bill_depth_mm, color = Cluster)) +
  geom_point() + 
  legend_pos
```

**Question 1**: Comparing the observed species plot with 3 species with the kmeans() cluster plot with 3 clusters, where does this "unsupervised" `kmeans()` technique (that does not use species to "fit" the model) produce similar versus different results? One or two sentences would suffice. Feel free to mention ranges of values along the axes.


```{r, eval=F, echo=F}
# **Task**: Highlight the "misclassified" points in the plot. _Hints: To get just the points misclassified, you can use `penguins_k$cluster != as.integer(penguins$species)`, which can feed as the second argument into `filter(penguins)`. To add another set of points to the ggplot, use `+ geom_point()` with arguments for: `data` with the additional points, `pch` [point shape](https://www.r-bloggers.com/2021/06/r-plot-pch-symbols-different-point-shapes-in-r/) with `fill=NA` for transparency and outline `color="red"`.

obs_mis <- penguins %>% 
  filter(penguins_k$cluster != as.integer(penguins$species))

ggplot(penguins, aes(bill_length_mm, bill_depth_mm, color = Cluster)) +
  geom_point() + 
  legend_pos +
  geom_point(data = obs_mis, color="red", fill=NA, pch=21)
```

### Plot Voronoi diagram of clustered `penguins`

This form of clustering assigns points to the cluster based on nearest centroid. You can see the breaks more clearly with a [Voronoi diagram](https://en.wikipedia.org/wiki/Voronoi_diagram).

```{r}
# define bounding box for geom_voronoi()
xr <- extendrange(range(penguins$bill_length_mm), f=0.1)
yr <- extendrange(range(penguins$bill_depth_mm), f=0.1)

box <- tribble(
  ~bill_length_mm, ~bill_depth_mm, ~group,
  xr[1], yr[1], 1,
  xr[1], yr[2], 1,
  xr[2], yr[2], 1,
  xr[2], yr[1], 1,
  xr[1], yr[1], 1) %>% 
  data.frame()

# cluster using kmeans
k <- 3  # number of clusters
penguins_k <- kmeans(
  penguins %>% 
    select(bill_length_mm, bill_depth_mm), 
  centers = k)

# extract cluster assignment per observation
Cluster = factor(penguins_k$cluster)

# extract cluster centers
ctrs <- as.data.frame(penguins_k$centers) %>% 
  mutate(Cluster = factor(1:k))

# plot points with voronoi diagram showing nearest centroid
ggplot(penguins, aes(bill_length_mm, bill_depth_mm, color = Cluster)) +
  geom_point() + 
  legend_pos +
  geom_voronoi(data = ctrs, aes(fill=Cluster), color = NA, alpha=0.5, 
    outline = box) + 
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  geom_point(data = ctrs, pch=23, cex=2, fill="black")
```

**BONUS Task**: Show the Voronoi diagram for fewer (`k=2`) and more (`k=8`) clusters to see how assignment to cluster centroids work.

```{r}
# cluster using kmeans
k2 <- 2  # number of clusters

iris_k2 <- kmeans(
  iris %>% 
    select(Petal.Length, Petal.Width), 
  centers = k2)

# extract cluster assignment per observation
Cluster2 = factor(iris_k2$cluster)

# extract cluster centers
ctrs2 <- as.data.frame(iris_k2$centers) %>% 
  mutate(Cluster2 = factor(1:k2))

# plot points with voronoi diagram showing nearest centroid
ggplot(iris, aes(Petal.Length, Petal.Width, color = Cluster2)) +
  geom_point() + 
  legend_pos +
  geom_voronoi(data = ctrs2, aes(fill=Cluster2), color = NA, alpha=0.5, outline = box) + 
  geom_point(data = ctrs2, pch=23, cex=2, fill="black") +
  labs(title = "Voronoi diagram for 2 clusters")
```

```{r}
# cluster using kmeans
k8 <- 8  # number of clusters

iris_k8 <- kmeans(
  iris %>% 
    select(Petal.Length, Petal.Width), 
  centers = k8)

# extract cluster assignment per observation
Cluster8 = factor(iris_k8$cluster)

# extract cluster centers
ctrs8 <- as.data.frame(iris_k8$centers) %>% 
  mutate(Cluster8 = factor(1:k8))

# plot points with voronoi diagram showing nearest centroid
ggplot(iris, aes(Petal.Length, Petal.Width, color = Cluster8)) +
  geom_point() + 
  legend_pos +
  geom_voronoi(data = ctrs8, aes(fill=Cluster8), color = NA, alpha=0.5, outline = box) + 
  geom_point(data = ctrs8, pch=23, cex=2, fill="black") +
  labs(title = "Voronoi diagram for 2 clusters")
```

## Hierarchical Clustering

Next, cluster sites according to species composition. Use the `dune` dataset from the `vegan` R package.

### Load `dune` dataset

```{r}
# load dune dataset from package vegan
data("dune")

# show documentation on dataset if interactive
if (interactive())
  help(dune)
```

**Bonus Question**: What are the rows and columns composed of in the `dune` data frame?

### Calculate Ecological Distances on `sites`

Before we calculate ecological distance between sites for `dune`, let's look at these metrics with a simpler dataset, like the example given in Chapter 8 by @kindtTreeDiversityAnalysis2005.

```{r}
sites <- tribble(
  ~site, ~sp1, ~sp2, ~sp3,
    "A",    1,    1,    0,
    "B",    5,    5,    0,
    "C",    0,    0,    1) %>% 
  column_to_rownames("site")
sites
```

```{r}
sites_manhattan <- vegdist(sites, method="manhattan")
sites_manhattan
```

```{r}
sites_euclidean <- vegdist(sites, method="euclidean")
sites_euclidean
```

```{r}
sites_bray <- vegdist(sites, method="bray")
sites_bray
```

**Question 7:** In your own words, how does Bray Curtis differ from Euclidean distance? 

### Bray-Curtis Dissimilarity on `sites` 

Let's take a closer look at the [Bray-Curtis Dissimilarity](https://en.wikipedia.org/wiki/Bray%E2%80%93Curtis_dissimilarity) distance:

$$
B_{ij} = 1 - \frac{2C_{ij}}{S_i + S_j}
$$

- $B_{ij}$: Bray-Curtis dissimilarity value between sites $i$ and $j$. \
1 = completely dissimilar (no shared species); 0 = identical.

- $C_{ij}$: sum of the lesser counts $C$ for shared species common to both sites $i$ and $j$

- $S_{i OR j}$: sum of all species counts $S$ for the given site $i$ or $j$

So to calculate Bray-Curtis for the example `sites`: 

- $B_{AB} = 1 - \frac{2 * (1 + 1)}{2 + 10} = 1 - 4/12 = 1 - 1/3 = 0.667$

- $B_{AC} = 1 - \frac{2 * 0}{2 + 1} = 1$

- $B_{BC} = 1 - \frac{2 * 0}{10 + 1} = 1$

### Agglomerative hierarchical clustering on `dune` 

See text to accompany code: _HOMLR_ [21.3.1 Agglomerative hierarchical clustering](https://bradleyboehmke.github.io/HOML/hierarchical.html#agglomerative-hierarchical-clustering).

```{r}
# Dissimilarity matrix
d <- vegdist(dune, method="bray")
dim(d)
as.matrix(d)[1:5, 1:5]

# Hierarchical clustering using Complete Linkage
hc1 <- hclust(d, method = "complete" )

# Dendrogram plot of hc1
plot(hc1, cex = 0.6, hang = -1)

```

**Question 9:** Which function comes first, vegdist() or hclust(), and why? See HOMLR 21.3.1 Agglomerative hierarchical clustering.

```{r}
# Compute agglomerative clustering with agnes
hc2 <- agnes(dune, method = "complete")

# Agglomerative coefficient
hc2$ac

# Dendrogram plot of hc2
plot(hc2, which.plot = 2)

# methods to assess
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

# function to compute coefficient
ac <- function(x) {
  agnes(dune, method = x)$ac
}

# get agglomerative coefficient for each linkage method
purrr::map_dbl(m, ac)

```

```{r}
# Compute ward linkage clustering with agnes
hc3 <- agnes(dune, method = "ward")

# Agglomerative coefficient
hc3$ac

# Dendrogram plot of hc3
plot(hc3, which.plot = 2)
```

**Question 11:** In your own words how does hclust() differ from agnes()? See HOMLR 21.3.1 Agglomerative hierarchical clustering and help documentation (?hclust(), ?agnes()).



**Question 13:** Of the 4 methods, which is the “best” model in terms of Agglomerative Coefficient?

### Divisive hierarchical clustering on `dune` 

See text to accompany code: _HOMLR_ [21.3.2 Divisive hierarchical clustering](https://bradleyboehmke.github.io/HOML/hierarchical.html#divisive-hierarchical-clustering).

```{r}
# compute divisive hierarchical clustering
hc4 <- diana(dune)

# Divise coefficient; amount of clustering structure found
hc4$dc
```

**Question 15:** In your own words how does agnes() differ from diana()? See HOMLR 21.3.1 Agglomerative hierarchical clustering, slides from Lecture 05. Clustering and help documentation (?agnes(), ?diana()).

### Determining optimal clusters

See text to accompany code: _HOMLR_ [21.4 Determining optimal clusters](https://bradleyboehmke.github.io/HOML/hierarchical.html#determining-optimal-clusters).

```{r}
# Plot cluster results
p1 <- fviz_nbclust(dune, FUN = hcut, method = "wss",  k.max = 10) +
  ggtitle("(A) Elbow method")

p2 <- fviz_nbclust(dune, FUN = hcut, method = "silhouette", k.max = 10) +
  ggtitle("(B) Silhouette method")

p3 <- fviz_nbclust(dune, FUN = hcut, method = "gap_stat", k.max = 10) +
  ggtitle("(C) Gap statistic")

# Display plots side by side
gridExtra::grid.arrange(p1, p2, p3, nrow = 1)
```

### Working with dendrograms

```{r}
# Construct dendorgram for the Ames housing example
hc5 <- hclust(d, method = "ward.D2" )
dend_plot <- fviz_dend(hc5)
dend_data <- attr(dend_plot, "dendrogram")
dend_cuts <- cut(dend_data, h = 8)
fviz_dend(dend_cuts$lower[[2]])

# Ward's method
hc5 <- hclust(d, method = "ward.D2" )

# Cut tree into 4 groups
k = 4
sub_grp <- cutree(hc5, k = k)

# Number of members in each cluster
table(sub_grp)

# Plot full dendogram
fviz_dend(
  hc5,
  k = k,
  horiz = TRUE,
  rect = TRUE,
  rect_fill = TRUE,
  rect_border = "jco",
  k_colors = "jco")
```

**Question 17:** How do the optimal number of clusters compare between methods for those with a dashed line?


**Question 18:** In dendrogram plots, which is the biggest determinant of relatedness between observations: the distance between observations along the labeled axes or the height of their shared connection? See HOMLR 21.5 Working with dendrograms.

# Ordination

```{r}
# get data
url <- "https://koalaverse.github.io/homlr/data/my_basket.csv"
my_basket <- readr::read_csv(url)
dim(my_basket)
```

```{r}
my_basket
```

# Performing PCA in R

```{r}
h2o.no_progress()  # turn off progress bars for brevity
h2o.init(max_mem_size = "5g")  # connect to H2O instance
```

```{r}
# convert data to h2o object
my_basket.h2o <- as.h2o(my_basket)

# run PCA
my_pca <- h2o.prcomp(
  training_frame = my_basket.h2o,
  pca_method = "GramSVD",
  k = ncol(my_basket.h2o), 
  transform = "STANDARDIZE", 
  impute_missing = TRUE,
  max_runtime_secs = 1000)
my_pca
```

**Question 20:** Why is the pca_method of “GramSVD” chosen over “GLRM”?  


**Question 21:** How many initial principal components are chosen with respect to dimensions of the input data?  


```{r}
my_pca@model$eigenvectors %>% 
  as.data.frame() %>% 
  mutate(feature = row.names(.)) %>%
  ggplot(aes(pc1, reorder(feature, pc1))) +
  geom_point()
```

**Question 23:** What category of grocery items contribute most to PC1? (These are related because they're bought most often together on a given grocery trip)

```{r}
my_pca@model$eigenvectors %>% 
  as.data.frame() %>% 
  mutate(feature = row.names(.)) %>%
  ggplot(aes(pc1, pc2, label = feature)) +
  geom_text()
```

**Question 25:** What category of grocery items contribute the least to PC1 but positively towards PC2?

# Eigenvalue criterion

```{r}
# Compute eigenvalues
eigen <- my_pca@model$importance["Standard deviation", ] %>%
  as.vector() %>%
  .^2
  
# Sum of all eigenvalues equals number of variables
sum(eigen)
```

```{r}
## [1] 42

# Find PCs where the sum of eigenvalues is greater than or equal to 1
which(eigen >= 1)
```

```{r}
# Extract PVE and CVE
ve <- data.frame(
  PC  = my_pca@model$importance %>% seq_along(),
  PVE = my_pca@model$importance %>% .[2,] %>% unlist(),
  CVE = my_pca@model$importance %>% .[3,] %>% unlist())

# Plot PVE and CVE
ve %>%
  tidyr::gather(metric, variance_explained, -PC) %>%
  ggplot(aes(PC, variance_explained)) +
  geom_point() +
  facet_wrap(~ metric, ncol = 1, scales = "free")
```

**Question 27:** How many principal components would you include to explain 90% of the total variance?


```{r}
# How many PCs required to explain at least 75% of total variability
min(which(ve$CVE >= 0.75))
```

```{r}
# Screee plot criterion
data.frame(
  PC  = my_pca@model$importance %>% seq_along,
  PVE = my_pca@model$importance %>% .[2,] %>% unlist()) %>%
  ggplot(aes(PC, PVE, group = 1, label = PC)) +
  geom_point() +
  geom_line() +
  geom_text(nudge_y = -.002)
```

**Question 29:** How many principal components to include up to the elbow of the PVE, i.e. the “elbow” before plateau of dimensions explaining the least variance?


**Question 30:** What are a couple of disadvantages to using PCA? See HOMLR 17.6 Final thoughts.

# Non-metric MultiDimensional Scaling (NMDS)
## Unconstrained Ordination on Species

```{r}
# vegetation and environment in lichen pastures from Vare et al (1995)
data("varespec") # species
data("varechem") # chemistry

varespec %>% tibble()
```

**Question 31:** What are the dimensions of the varespec data frame and what do rows versus columns represent?


```{r}
vare.dis <- vegdist(varespec)
vare.mds0 <- monoMDS(vare.dis)

stressplot(vare.mds0)
```

**Question 33:** The “stress” in a stressplot represents the difference between the observed input distance versus the fitted ordination distance. How much better is the non-metric (i.e., NMDS) fit versus a linear fit (as with PCA) in terms of \(R^2\)?


```{r}
ordiplot(vare.mds0, type = "t")
```

**Question 35:** What two sites are most dissimilar based on species composition for the first component MDS1? And two more most dissimilar sites for the second component MDS2?


```{r}
vare.mds <- metaMDS(varespec, trace = FALSE)
vare.mds
```

```{r}
plot(vare.mds, type = "t")
```

**Question 38:** What is the basic difference between metaMDS and monoMDS()? 


# Overlay with Environment

### Environmental interpretation
- Vector fitting <br>
- Surface fitting <br>

```{r}
ef <- envfit(vare.mds, varechem, permu = 999)
ef
```

```{r}
plot(vare.mds, display = "sites")
plot(ef, p.max = 0.05)
```

**Question 40:** What two soil chemistry elements have the strongest negative relationship with NMDS1 that is based on species composition?

```{r}
ef <- envfit(vare.mds ~ Al + Ca, data = varechem)

plot(vare.mds, display = "sites")
plot(ef)

tmp <- with(varechem, ordisurf(vare.mds, Al, add = TRUE))
ordisurf(vare.mds ~ Ca, data=varechem, add = TRUE, col = "green4")
```

**Question 42:** Which of the two NMDS axes differentiates Ca the most, i.e. has the highest value given by the contours at the end (and not middle) of the axis?

**Answer:**


# Constrained Ordination on Species and Environment
See supporting text in vegantutor.pdf:

## Constrained ordination
#### Model specification
Technically, this uses another technique cca, or canonical correspondence analysis.

```{r}
# ordinate on species constrained by three soil elements
vare.cca <- cca(varespec ~ Al + P + K, varechem)
vare.cca
```

**Question 43:** What is the difference between “constrained” versus “unconstrained” ordination within ecological context?


```{r}
# plot ordination
plot(vare.cca)
```

```{r}
# plot 3 dimensions
ordiplot3d(vare.cca, type = "h")
```

```{r}
if (interactive()){
  ordirgl(vare.cca)
}
```

**Question 45:** What sites are most differentiated by CCA1, i.e. furthest apart along its axis, based on species composition AND the environment? What is the strongest environmental vector for CCA1, i.e. longest environmental vector in the direction of the CCA1 axes?

**Answer:** 
Plot 621
Sites = black 

what are the two most diff = 28, 4

Envir vector - blue, what changes the most along xaxis